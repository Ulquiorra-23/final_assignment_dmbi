{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries\n",
    "import os\n",
    "\n",
    "# Importing third party libraries\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "# Importing custom libraries\n",
    "from tools.sql_tools import write_to_database\n",
    "from tools.logs import log_wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 06:57:14,429 - __main__ - INFO - sales_phases_funnel_df created\n",
      "2025-03-08 06:57:14,438 - __main__ - INFO - zipcodedf created\n",
      "2025-03-08 06:57:16,225 - __main__ - INFO - meteo_df created\n",
      "2025-03-08 06:57:16,273 - __main__ - INFO - There are 0 duplicate rows in sales_funnel_df before duplicate cleaning\n",
      "2025-03-08 06:57:16,276 - __main__ - INFO - There are 0 duplicate rows in zipcode_df before duplicate cleaning\n",
      "2025-03-08 06:57:17,232 - __main__ - INFO - There are 0 duplicate rows in meteo_df before duplicate cleaning\n",
      "2025-03-08 06:57:18,247 - __main__ - INFO - There are 0 duplicate rows in sales_funnel_df after duplicate cleaning\n",
      "2025-03-08 06:57:18,251 - __main__ - INFO - There are 0 duplicate rows in zipcode_df after duplicate cleaning\n",
      "2025-03-08 06:57:19,211 - __main__ - INFO - There are 0 duplicate rows in meteo_df after duplicate cleaning\n",
      "2025-03-08 06:57:19,220 - __main__ - INFO - Unreachable leads deleted\n",
      "2025-03-08 06:57:19,222 - __main__ - INFO - outliers_df dataframe created\n",
      "2025-03-08 06:57:19,222 - __main__ - INFO - Number of outliers: 1096\n",
      "2025-03-08 06:57:19,225 - __main__ - INFO - outliers removed from sales_phases_funnel_df\n",
      "2025-03-08 06:57:19,225 - __main__ - INFO - outliers_df dataframe added to list_of_dfs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom tools.cleaning import dataFrameCreate, dropDupli, delete_unreachable_leads, delete_outliers\\n\\n\\n\\nlist_of_dfs = dataFrameCreate()\\nlist_of_dfs = dropDupli(list_of_dfs)\\nlist_of_dfs = delete_unreachable_leads(list_of_dfs) #this one we may exclude\\nlist_of_dfs = delete_outliers(list_of_dfs)\\n\\nprint(list_of_dfs)\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------FOR THE BOIS----------------------\n",
    "\n",
    "#--------LIBRARIES FOR THE BOIS (THE ONES I USED, THE USUAL ONES)-----------\n",
    "\n",
    "#COOL LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tools.logs import log_wrap\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Create logger instance\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SET PATHS OF 3 COOL CSVs\n",
    "FILENAME_sales_phases_funnel_df = os.path.join(os.getcwd(), r'data/sale_phases_funnel.csv')\n",
    "FILENAME_zipcode_df = os.path.join(os.getcwd(), r'data/zipcode_eae.csv')\n",
    "FILENAME_meteo_df = os.path.join(os.getcwd(), r'data/meteo_eae.csv')\n",
    "\n",
    "\n",
    "#SETTING TYPES\n",
    "#Sales\n",
    "SALES_TYPES = {'LEAD_ID':'str','FINANCING_TYPE':'str',\n",
    "                    'CURRENT_PHASE':'str','PHASE_PRE_KO':'str',\n",
    "                    'IS_MODIFIED':'bool','ZIPCODE':'str', \n",
    "                    'VISITING_COMPANY': 'str', 'KO_REASON': 'str', \n",
    "                    'INSTALLATION_PEAK_POWER_KW': 'float64', \n",
    "                    'INSTALLATION_PRICE': 'float', \n",
    "                    'N_PANELS': 'int', 'CUSOMER_TYPE': 'str' }\n",
    "\n",
    "#Zipcosdes\n",
    "ZIPCODE_TYPES = {'ZIPCODE':'str','ZC_LATITUDE':'float64',\n",
    "                    'ZC_LONGITUDE':'float64','AUTONOMOUS_COMMUNITY':'str',\n",
    "                    'AUTONOMOUS_COMMUNITY_NK':'str','PROVINCE':'str'}\n",
    "\n",
    "#Meteo\n",
    "METEO_TYPES = {'temperature': 'float', 'relative_humidity': 'float', \n",
    "            'precipitation_rate': 'float', 'wind_speed': 'float', \n",
    "            'zipcode': 'str' \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------FUNCTIONS FOR THE BOIS-----------\n",
    "\n",
    "#CREATE 3 COOL DATAFRAMES FUNCTION\n",
    "#Creates 3 super cool dataframes from the CSVs with the data types set from the start.\n",
    "def dataFrameCreate():\n",
    "\n",
    "    #SALES FUNNEL DATAFRAME\n",
    "\n",
    "    #Dictionary with data types\n",
    "   \n",
    "\n",
    "    #Reading CSV to create dataframe with datatypes implemented from dictionary and additional date time datatypes.\n",
    "    sales_phases_funnel_df = pd.read_csv(\n",
    "        FILENAME_sales_phases_funnel_df, \n",
    "        delimiter=';', \n",
    "        dtype=SALES_TYPES,\n",
    "        parse_dates=['OFFER_SENT_DATE', 'CONTRACT_1_DISPATCH_DATE', \n",
    "                    'CONTRACT_2_DISPATCH_DATE', \n",
    "                    'CONTRACT_1_SIGNATURE_DATE', \n",
    "                    'CONTRACT_2_SIGNATURE_DATE',\n",
    "                    'VISIT_DATE',\n",
    "                    'TECHNICAL_REVIEW_DATE',\n",
    "                    'PROJECT_VALIDATION_DATE',\n",
    "                    'SALE_DISMISSAL_DATE',\n",
    "                    'KO_DATE'],\n",
    "                    \n",
    "        dayfirst=True  # This replaces the dayfirst=True in your to_datetime call\n",
    "    )\n",
    "\n",
    "    logger.info('sales_phases_funnel_df created')\n",
    "\n",
    "\n",
    "\n",
    "    #ZIPCODE DATAFRAME\n",
    "\n",
    "    # Reading CSV to create dataframe with datatypes implemented from dictionary\n",
    "    zipcode_df = pd.read_csv(FILENAME_zipcode_df, delimiter=',', dtype=ZIPCODE_TYPES)\n",
    "\n",
    "\n",
    "    logger.info('zipcodedf created')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #METEO DATAFRAME\n",
    "\n",
    "    # Reading CSV to create dataframe with datatypes implemented from dictionary and \n",
    "    # additional date time datatype formatted to match the ones from the sales dataframe.\n",
    "    meteo_df = pd.read_csv(FILENAME_meteo_df, delimiter=';',\n",
    "        dtype=METEO_TYPES, parse_dates=['date'],  # Replace with actual column name\n",
    "        date_format='%Y/%m/%d %H:%M:%S.%f'  # This matches your input format\n",
    "    )\n",
    "\n",
    "    logger.info('meteo_df created')\n",
    "    list_of_dfs = [sales_phases_funnel_df, zipcode_df, meteo_df]\n",
    "\n",
    "    return list_of_dfs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "list_of_dfs = dataFrameCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--GLOBAL CLEANING FUNCTION--\n",
    "\n",
    "\n",
    "#DROPPING DUPLICATES FOR ALL DATAFRAMES\n",
    "\n",
    "#creating the drop duplicate function\n",
    "def dropDupli(dfs):\n",
    "    #log\n",
    "    logger.info(f'There are {dfs[0].duplicated().sum()} duplicate rows in sales_funnel_df before duplicate cleaning') \n",
    "    logger.info(f'There are {dfs[1].duplicated().sum()} duplicate rows in zipcode_df before duplicate cleaning')\n",
    "    logger.info(f'There are {dfs[2].duplicated().sum()} duplicate rows in meteo_df before duplicate cleaning')  \n",
    "    #DroppingDupli\n",
    "    dfs[0].drop_duplicates(inplace=True)\n",
    "    dfs[1].drop_duplicates(inplace=True)\n",
    "    dfs[2].drop_duplicates(inplace=True)\n",
    "    # Log after\n",
    "    logger.info(f'There are {dfs[0].duplicated().sum()} duplicate rows in sales_funnel_df after duplicate cleaning') \n",
    "    logger.info(f'There are {dfs[1].duplicated().sum()} duplicate rows in zipcode_df after duplicate cleaning')\n",
    "    logger.info(f'There are {dfs[2].duplicated().sum()} duplicate rows in meteo_df after duplicate cleaning')\n",
    "\n",
    "    return dfs  # Return dfs instead of undefined variables\n",
    "\n",
    "\n",
    "\n",
    "list_of_dfs = dropDupli(list_of_dfs)\n",
    "\n",
    "\n",
    "\n",
    "# --SALES FUNNEL DATAFRAME CLEANING FUCTIONS--\n",
    "\n",
    "\n",
    "#DELETE UNUSABLE LEADS FUNCTION \n",
    "\n",
    "# Drop rows where KO_REASON is \"Unreachable\"\n",
    "def delete_unreachable_leads(dfs):\n",
    "    dfs[0] = dfs[0][~((dfs[0]['CURRENT_PHASE'] == 'KO') & (dfs[0]['KO_REASON'] == 'Unreachable'))]\n",
    "    # Reset the index of the updated DataFrame\n",
    "    dfs[0].reset_index(drop=True, inplace=True)\n",
    "    logger.info('Unreachable leads deleted')\n",
    "    return dfs\n",
    "\n",
    "list_of_dfs = delete_unreachable_leads(list_of_dfs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# REMOVE OUTLIERS FUNCTION\n",
    "\n",
    "\n",
    "def delete_outliers(dfs):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = dfs[0]['INSTALLATION_PRICE'].quantile(0.25)\n",
    "    Q3 = dfs[0]['INSTALLATION_PRICE'].quantile(0.75)\n",
    "    \n",
    "\n",
    "    # Calculate the Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Identify outliers inside a new Data Frame\n",
    "    outliers_df = dfs[0][(dfs[0]['INSTALLATION_PRICE'] < (Q1 - 1.5 * IQR)) | \n",
    "                                     (dfs[0]['INSTALLATION_PRICE'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "    # Print the number of outliers\n",
    "    logger.info('outliers_df dataframe created')\n",
    "    logger.info(f'Number of outliers: {len(outliers_df)}')\n",
    "    \n",
    "    # Update sales_phases_funnel_df to exclude the outliers\n",
    "    sales_phases_funnel_df = dfs[0][~((dfs[0]['INSTALLATION_PRICE'] < (Q1 - 1.5 * IQR)) | \n",
    "                                                  (dfs[0]['INSTALLATION_PRICE'] > (Q3 + 1.5 * IQR)))]\n",
    "    logger.info('outliers removed from sales_phases_funnel_df')\n",
    "    dfs.append(outliers_df)\n",
    "    logger.info(f'outliers_df dataframe added to list_of_dfs')\n",
    "    return dfs\n",
    "    \n",
    "\n",
    "\n",
    "list_of_dfs = delete_outliers(list_of_dfs)\n",
    "\n",
    "\n",
    "#This is how you would use it in th final etl:\n",
    "'''\n",
    "from tools.cleaning import dataFrameCreate, dropDupli, delete_unreachable_leads, delete_outliers\n",
    "\n",
    "\n",
    "\n",
    "list_of_dfs = dataFrameCreate()\n",
    "list_of_dfs = dropDupli(list_of_dfs)\n",
    "list_of_dfs = delete_unreachable_leads(list_of_dfs) #this one we may exclude\n",
    "list_of_dfs = delete_outliers(list_of_dfs)\n",
    "\n",
    "print(list_of_dfs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Column Names\n",
    "FINAL_NAMES_WEATHER = {'date':'year','temperature': 'avg_temperature', 'relative_humidity': 'avg_relative_humidity',\n",
    "                                               'precipitation_rate':'avg_precipitation_rate','wind_speed':'avg_wind_speed'}\n",
    "\n",
    "# Final Columns\n",
    "FINAL_COLS_SALES = ['sales_id','zipcode_id','lead_id','financing_type','current_phase','phase_pre_ko',\n",
    "              'is_modified','offer_sent_date','contract_1_dispatch_date','contract_2_dispatch_date','contract_1_signature_date',\n",
    "              'contract_2_signature_date','most_recent_contract_signature','visit_date','technical_review_date',\n",
    "              'project_validation_date','sale_dismissal_date','ko_date','visiting_company','ko_reason',\n",
    "              'installation_peak_power_kw','installation_price','n_panels','cusomer_type']\n",
    "FINAL_COLS_WEATHER=['weather_id','zipcode_id','year','avg_temperature','avg_relative_humidity','avg_precipitation_rate',\n",
    "                    'avg_wind_speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_wrap\n",
    "def transform_data(data: list, logger) -> list:\n",
    "    '''\n",
    "    Takes a list of dfs as arguments of size 3 and returns a list of transformed dataframes\n",
    "    Order:\n",
    "    [0] = Sales\n",
    "    [1] = Zipcode\n",
    "    [2] = Weather\n",
    "    '''\n",
    "    try:\n",
    "        logger.info('Reading Dataframes...')\n",
    "        sales_fact_df_raw = data[0]\n",
    "        logger.info(f'Sales Data has {len(sales_fact_df_raw)} records.')\n",
    "        zipcode_dim_df_raw = data[1]\n",
    "        logger.info(f'Zipcode Data has {len(zipcode_dim_df_raw)} records.')\n",
    "        weather_dim_df_raw = data[2]\n",
    "        logger.info(f'Weather Data has {len(weather_dim_df_raw)} records.')\n",
    "        \n",
    "        logger.info(f'Processing Transformations...')\n",
    "        sales_fact_df_raw.columns = sales_fact_df_raw.columns.str.lower()\n",
    "        zipcode_dim_df_raw.columns = zipcode_dim_df_raw.columns.str.lower()\n",
    "        weather_dim_df_raw.columns = weather_dim_df_raw.columns.str.lower()\n",
    "        \n",
    "        logger.info(f'Creating a PK in zipcode_dim_df_raw...')\n",
    "        zipcode_dim_df_raw.insert(0,'zipcode_id',range(1, len(zipcode_dim_df_raw) + 1))\n",
    "        zipcode_dim_df_raw['zipcode_id'] = zipcode_dim_df_raw['zipcode_id'].astype('int32')\n",
    "        zipcode_dim_df = zipcode_dim_df_raw\n",
    "        \n",
    "        logger.info(f'Grouping weather_dim_df_raw...')\n",
    "        weather_dim_df_raw['date'] = weather_dim_df_raw['date'].dt.year\n",
    "        weather_dim_df_raw = weather_dim_df_raw.groupby(['date','zipcode']).mean().reset_index()\n",
    "        \n",
    "        logger.info(f'Adding FK zipcode_id in weather table...')\n",
    "        weather_dim_df = pd.merge(weather_dim_df_raw,zipcode_dim_df_raw,on= 'zipcode', how='left')\n",
    "        \n",
    "        logger.info(f'Dropping null zipcode_id from weather table...')\n",
    "        weather_dim_df = weather_dim_df.dropna()\n",
    "        \n",
    "        logger.info(f'Creating a PK in weather_dim_df_raw...')\n",
    "        weather_dim_df.insert(0,'weather_id',range(1, len(weather_dim_df) + 1))\n",
    "        weather_dim_df['weather_id'] = weather_dim_df['weather_id'].astype('int32')\n",
    "        \n",
    "        logger.info(f'Creating a PK in sales_fact_df_raw...')\n",
    "        sales_fact_df_raw.insert(0,'sales_id',range(1, len(sales_fact_df_raw) + 1))\n",
    "        sales_fact_df_raw['sales_id'] = sales_fact_df_raw['sales_id'].astype('int32')\n",
    "        \n",
    "        logger.info(f'Adding calculated column most_recent_contract_signature to sales_fact_df...')\n",
    "        sales_fact_df_raw.insert(16,'most_recent_contract_signature', \\\n",
    "            sales_fact_df_raw[['contract_1_signature_date', 'contract_2_signature_date']].max(axis=1))\n",
    "\n",
    "        logger.info(f'Adding FK zipcode_id in sales table...')\n",
    "        sales_fact_df = pd.merge(sales_fact_df_raw, zipcode_dim_df_raw, on='zipcode', how='left')\n",
    "        \n",
    "        logger.info(f'Handling column types, names and selection...')\n",
    "        weather_dim_df = weather_dim_df.rename(columns=FINAL_NAMES_WEATHER)\n",
    "        weather_dim_df = weather_dim_df[FINAL_COLS_WEATHER]\n",
    "        sales_fact_df = sales_fact_df[FINAL_COLS_SALES]        \n",
    "        \n",
    "        logger.info(f'Packing data for loading...')\n",
    "        list_of_transformed_dfs = [zipcode_dim_df,weather_dim_df, sales_fact_df]\n",
    "        \n",
    "        return list_of_transformed_dfs\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f'Transformation error: {e}', exc_info=True)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 06:57:19,248 - transform_data - INFO - Executing function: transform_data\n",
      "2025-03-08 06:57:19,248 - transform_data - INFO - Reading Dataframes...\n",
      "2025-03-08 06:57:19,249 - transform_data - INFO - Sales Data has 28864 records.\n",
      "2025-03-08 06:57:19,250 - transform_data - INFO - Zipcode Data has 11407 records.\n",
      "2025-03-08 06:57:19,251 - transform_data - INFO - Weather Data has 4114206 records.\n",
      "2025-03-08 06:57:19,251 - transform_data - INFO - Processing Transformations...\n",
      "2025-03-08 06:57:19,253 - transform_data - INFO - Creating a PK in zipcode_dim_df_raw...\n",
      "2025-03-08 06:57:19,255 - transform_data - INFO - Grouping weather_dim_df_raw...\n",
      "2025-03-08 06:57:19,513 - transform_data - INFO - Adding FK zipcode_id in weather table...\n",
      "2025-03-08 06:57:19,519 - transform_data - INFO - Dropping null zipcode_id from weather table...\n",
      "2025-03-08 06:57:19,521 - transform_data - INFO - Creating a PK in weather_dim_df_raw...\n",
      "2025-03-08 06:57:19,521 - transform_data - INFO - Creating a PK in sales_fact_df_raw...\n",
      "2025-03-08 06:57:19,522 - transform_data - INFO - Adding calculated column most_recent_contract_signature to sales_fact_df...\n",
      "2025-03-08 06:57:19,525 - transform_data - INFO - Adding FK zipcode_id in sales table...\n",
      "2025-03-08 06:57:19,531 - transform_data - INFO - Handling column types, names and selection...\n",
      "2025-03-08 06:57:19,535 - transform_data - INFO - Packing data for loading...\n",
      "2025-03-08 06:57:19,536 - transform_data - INFO - Completed function: transform_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[       zipcode_id zipcode  zc_latitude  zc_longitude autonomous_community  \\\n",
       " 0               1   01001      42.8500       -2.6667           Pais Vasco   \n",
       " 1               2   07119      39.6833        2.7000             Baleares   \n",
       " 2               3   07110      39.6833        2.7000             Baleares   \n",
       " 3               4   07109      39.7833        2.7333             Baleares   \n",
       " 4               5   07108      39.8000        2.6833             Baleares   \n",
       " ...           ...     ...          ...           ...                  ...   \n",
       " 11402       11403   49543      41.6667       -6.0333      Castilla - Leon   \n",
       " 11403       11404   49542      41.7167       -6.1500      Castilla - Leon   \n",
       " 11404       11405   49541      41.6833       -6.0833      Castilla - Leon   \n",
       " 11405       11406   49540      41.7500       -5.9833      Castilla - Leon   \n",
       " 11406       11407   49539      41.6167       -5.5333      Castilla - Leon   \n",
       " \n",
       "       autonomous_community_nk       province  \n",
       " 0                          PV          Álava  \n",
       " 1                          IB  Illes Balears  \n",
       " 2                          IB  Illes Balears  \n",
       " 3                          IB  Illes Balears  \n",
       " 4                          IB  Illes Balears  \n",
       " ...                       ...            ...  \n",
       " 11402                      CL         Zamora  \n",
       " 11403                      CL         Zamora  \n",
       " 11404                      CL         Zamora  \n",
       " 11405                      CL         Zamora  \n",
       " 11406                      CL         Zamora  \n",
       " \n",
       " [11407 rows x 7 columns],\n",
       "        weather_id  zipcode_id  year  avg_temperature  avg_relative_humidity  \\\n",
       " 14              1         1.0  2024        11.562535              78.391780   \n",
       " 15              2      1000.0  2024        11.562535              78.391780   \n",
       " 16              3       999.0  2024        11.562535              78.391780   \n",
       " 17              4       998.0  2024        11.562535              78.391780   \n",
       " 18              5       997.0  2024        11.562535              78.391780   \n",
       " ...           ...         ...   ...              ...                    ...   \n",
       " 11216       10465     11024.0  2024        15.899365              60.551156   \n",
       " 11217       10466     11023.0  2024        16.341548              59.216941   \n",
       " 11218       10467     11022.0  2024        16.304705              59.403024   \n",
       " 11219       10468     11021.0  2024        16.036900              60.103512   \n",
       " 11220       10469     11020.0  2024        13.940463              64.630565   \n",
       " \n",
       "        avg_precipitation_rate  avg_wind_speed  \n",
       " 14                   0.000033        2.933424  \n",
       " 15                   0.000033        2.933424  \n",
       " 16                   0.000033        2.933424  \n",
       " 17                   0.000033        2.933424  \n",
       " 18                   0.000033        2.933424  \n",
       " ...                       ...             ...  \n",
       " 11216                0.000017        4.743960  \n",
       " 11217                0.000015        5.131758  \n",
       " 11218                0.000015        4.889607  \n",
       " 11219                0.000016        4.762858  \n",
       " 11220                0.000028        3.124402  \n",
       " \n",
       " [10469 rows x 7 columns],\n",
       "        sales_id  zipcode_id   lead_id financing_type      current_phase  \\\n",
       " 0             1         NaN  C8877823           cash  Validated project   \n",
       " 1             2      6673.0  C2068654           cash                 KO   \n",
       " 2             3      4678.0  C1925058           cash                 KO   \n",
       " 3             4      3828.0  C3357155           cash                 KO   \n",
       " 4             5     10747.0  C5104785           cash                 KO   \n",
       " ...         ...         ...       ...            ...                ...   \n",
       " 28859     28860      9302.0  C2986619           cash                 KO   \n",
       " 28860     28861         NaN  C1071478           cash                 KO   \n",
       " 28861     28862     10423.0  C8350262       financed  Validated project   \n",
       " 28862     28863      3489.0  C5247567           cash                 KO   \n",
       " 28863     28864      7271.0  C4570179           cash                 KO   \n",
       " \n",
       "             phase_pre_ko  is_modified offer_sent_date  \\\n",
       " 0      Validated project        False      2024-01-01   \n",
       " 1          Initial Offer        False      2024-01-01   \n",
       " 2       Commercial visit        False      2024-01-01   \n",
       " 3          Initial Offer        False      2024-01-01   \n",
       " 4          Initial Offer        False      2024-01-01   \n",
       " ...                  ...          ...             ...   \n",
       " 28859            Winback        False      2024-12-30   \n",
       " 28860      Initial Offer        False      2024-12-30   \n",
       " 28861  Validated project        False      2024-12-30   \n",
       " 28862      Initial Offer        False      2024-12-30   \n",
       " 28863      Initial Offer        False      2024-12-30   \n",
       " \n",
       "       contract_1_dispatch_date contract_2_dispatch_date  ...  \\\n",
       " 0                   2024-01-03                      NaT  ...   \n",
       " 1                          NaT                      NaT  ...   \n",
       " 2                          NaT                      NaT  ...   \n",
       " 3                          NaT                      NaT  ...   \n",
       " 4                          NaT                      NaT  ...   \n",
       " ...                        ...                      ...  ...   \n",
       " 28859                      NaT                      NaT  ...   \n",
       " 28860                      NaT                      NaT  ...   \n",
       " 28861               2025-01-08               2025-01-10  ...   \n",
       " 28862                      NaT                      NaT  ...   \n",
       " 28863                      NaT                      NaT  ...   \n",
       " \n",
       "       technical_review_date project_validation_date sale_dismissal_date  \\\n",
       " 0                2024-01-10              2024-02-22                 NaT   \n",
       " 1                       NaT                     NaT                 NaT   \n",
       " 2                       NaT                     NaT                 NaT   \n",
       " 3                       NaT                     NaT                 NaT   \n",
       " 4                       NaT                     NaT                 NaT   \n",
       " ...                     ...                     ...                 ...   \n",
       " 28859                   NaT                     NaT                 NaT   \n",
       " 28860                   NaT                     NaT                 NaT   \n",
       " 28861            2025-01-22              2025-01-22                 NaT   \n",
       " 28862                   NaT                     NaT                 NaT   \n",
       " 28863                   NaT                     NaT                 NaT   \n",
       " \n",
       "          ko_date visiting_company                                  ko_reason  \\\n",
       " 0            NaT         Internal                                        NaN   \n",
       " 1     2024-01-02         Internal                            Useless Contact   \n",
       " 2     2024-01-03         Internal  Product. Vertical Building / Condominiums   \n",
       " 3     2024-01-04         Internal                            Useless Contact   \n",
       " 4     2024-01-04         Internal                            Useless Contact   \n",
       " ...          ...              ...                                        ...   \n",
       " 28859 2025-03-20         Internal            Useless Contact. Not Interested   \n",
       " 28860 2025-01-08         Internal            Useless Contact. Not Interested   \n",
       " 28861        NaT         Internal                                        NaN   \n",
       " 28862 2025-01-02         Internal            Useless Contact. Not Interested   \n",
       " 28863 2025-01-01         Internal    Useless Contact. Incorrect Phone Number   \n",
       " \n",
       "       installation_peak_power_kw installation_price n_panels  \\\n",
       " 0                          4.800            9562.77       12   \n",
       " 1                          1.600            4650.92        4   \n",
       " 2                          3.600            7438.55        9   \n",
       " 3                          3.200            6580.33        8   \n",
       " 4                          2.800            5937.57        7   \n",
       " ...                          ...                ...      ...   \n",
       " 28859                      1.515            5944.00        3   \n",
       " 28860                      3.535            8522.83        7   \n",
       " 28861                      6.060           11457.01       12   \n",
       " 28862                      2.020            6743.50        4   \n",
       " 28863                      5.555           10520.33       11   \n",
       " \n",
       "                cusomer_type  \n",
       " 0      Individual household  \n",
       " 1      Individual household  \n",
       " 2       Community of owners  \n",
       " 3                       SME  \n",
       " 4      Individual household  \n",
       " ...                     ...  \n",
       " 28859  Individual household  \n",
       " 28860                   SME  \n",
       " 28861  Individual household  \n",
       " 28862                   SME  \n",
       " 28863  Individual household  \n",
       " \n",
       " [28864 rows x 24 columns]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_data(list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries\n",
    "import os\n",
    "\n",
    "# Importing third party libraries\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(os.getcwd(), 'creds.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILENAME, \"r\") as file:\n",
    "    creds = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_CREATE_ZIPCODE_DIM_TABLE ='''\n",
    "CREATE TABLE IF NOT EXISTS zipcode_dim (\n",
    "  zipcode_id int NOT NULL AUTO_INCREMENT PRIMARY KEY,\n",
    "  zipcode varchar(50) NOT NULL,\n",
    "  zc_latitude float NOT NULL,\n",
    "  zc_longitude float NOT NULL,\n",
    "  autonomous_community varchar(50) NOT NULL,\n",
    "  autonomous_community_nk varchar(50) NOT NULL,\n",
    "  province varchar(50) NOT NULL\n",
    ");\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_CREATE_WEATHER_DIM_TABLE = '''\n",
    "CREATE TABLE IF NOT EXISTS weather_dim (\n",
    "  weather_id int NOT NULL AUTO_INCREMENT PRIMARY KEY,\n",
    "  zipcode_id int NOT NULL,\n",
    "  year int NOT NULL,\n",
    "  avg_temperature float NOT NULL,\n",
    "  avg_relative_humidity float NOT NULL,\n",
    "  avg_precipitation_rate float NOT NULL,\n",
    "  avg_wind_speed float NOT NULL,\n",
    "  Foreign Key (zipcode_id) references zipcode_dim(zipcode_id)\n",
    ");\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_CREATE_SALES_FT_TABLE = '''\n",
    "CREATE TABLE IF NOT EXISTS sales_fact (\n",
    "  sales_id int NOT NULL AUTO_INCREMENT PRIMARY KEY,\n",
    "  zipcode_id int NOT NULL,\n",
    "  lead_id varchar(50) NOT NULL,\n",
    "  financing_type varchar(50) NOT NULL,\n",
    "  current_phase varchar(50) NOT NULL,\n",
    "  phase_pre_ko varchar(50) NOT NULL,\n",
    "  is_modified int,\n",
    "  offer_sent_date date NOT NULL,\n",
    "  contract_1_dispatch_date date NOT NULL,\n",
    "  contract_2_dispatch_date date NOT NULL,\n",
    "  contract_1_signature_date date NOT NULL,\n",
    "  contract_2_signature_date date NOT NULL,\n",
    "  most_recent_contract_signature_date date NOT NULL,\n",
    "  visit_date date NOT NULL,\n",
    "  technical_review_date date NOT NULL,\n",
    "  project_validation_date date NOT NULL,\n",
    "  sale_dismissal_date date NOT NULL,\n",
    "  ko_date date NOT NULL,\n",
    "  visiting_company varchar(50) NOT NULL,\n",
    "  ko_reason varchar(50) NOT NULL,\n",
    "  installation_peak_power_kwf float NOT NULL,\n",
    "  installation_price float NOT NULL,\n",
    "  n_panels smallint,\n",
    "  customer_type varchar(50) NOT NULL,\n",
    "  Foreign Key (zipcode_id) references zipcode_dim(zipcode_id)\n",
    ");\n",
    "'''\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    connection = mysql.connector.connect(\n",
    "        user = creds['mysql-db']['username'],\n",
    "        password = creds['mysql-db']['password'],\n",
    "        host = creds['mysql-db']['host'],\n",
    "        database = creds['mysql-db']['database'],\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    cursor.execute(QUERY_CREATE_ZIPCODE_DIM_TABLE)\n",
    "    cursor.execute(QUERY_CREATE_WEATHER_DIM_TABLE)\n",
    "    cursor.execute(QUERY_CREATE_SALES_FT_TABLE)\n",
    "    connection.commit()\n",
    "    print(\"Table structures created successfully.\")\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_transformed_dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dfs_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzipcode_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mlist_of_transformed_dfs\u001b[49m[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: list_of_transformed_dfs[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_fact\u001b[39m\u001b[38;5;124m\"\u001b[39m: list_of_transformed_dfs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      5\u001b[0m     }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_transformed_dfs' is not defined"
     ]
    }
   ],
   "source": [
    "dfs_dict = {\n",
    "        \"zipcode_dim\": list_of_transformed_dfs[0],\n",
    "        \"weather_dim\": list_of_transformed_dfs[1],\n",
    "        \"sales_fact\": list_of_transformed_dfs[2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_database(dfs_dict, if_exists='append'):\n",
    "    \"\"\"\n",
    "    Write a dataframe into a MySql table.\n",
    "\n",
    "    Args:\n",
    "        dfs_dict: The list of tables to load to along with the dfs to insert\n",
    "        if_exists (str): Default 'append'\n",
    "    \"\"\"\n",
    "\n",
    "    _db_user = creds['username']\n",
    "    _db_password = creds['password']\n",
    "    _db_host = creds['host']\n",
    "    _db_name = creds['database']\n",
    "    engine = create_engine(f\"mysql+pymysql://{_db_user}:{_db_password}@{_db_host}:3306/{_db_name}\")\n",
    "    with engine.connect() as connection:\n",
    "        for table_name, df in dfs_dict.items():\n",
    "            if isinstance(df, pd.DataFrame): \n",
    "                df.to_sql(table_name, con=connection, if_exists=if_exists, index=False)\n",
    "                print(f\"Data successfully inserted into {table_name}\")\n",
    "            else:\n",
    "                print(f\"Skipping {table_name}: Not a valid DataFrame\")\n",
    "    \n",
    "    # return logger.info(\"Completed uploading all data..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table structures created successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dfs_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     create_table()\n\u001b[0;32m----> 3\u001b[0m     write_to_database(\u001b[43mdfs_dict\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs_dict' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    create_table()\n",
    "    write_to_database(dfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 07:00:40,878 - __main__ - INFO - sales_phases_funnel_df created\n",
      "2025-03-08 07:00:40,887 - __main__ - INFO - zipcodedf created\n",
      "2025-03-08 07:00:42,655 - __main__ - INFO - meteo_df created\n",
      "2025-03-08 07:00:42,693 - __main__ - INFO - There are 0 duplicate rows in sales_funnel_df before duplicate cleaning\n",
      "2025-03-08 07:00:42,698 - __main__ - INFO - There are 0 duplicate rows in zipcode_df before duplicate cleaning\n",
      "2025-03-08 07:00:43,661 - __main__ - INFO - There are 0 duplicate rows in meteo_df before duplicate cleaning\n",
      "2025-03-08 07:00:44,649 - __main__ - INFO - There are 0 duplicate rows in sales_funnel_df after duplicate cleaning\n",
      "2025-03-08 07:00:44,651 - __main__ - INFO - There are 0 duplicate rows in zipcode_df after duplicate cleaning\n",
      "2025-03-08 07:00:45,594 - __main__ - INFO - There are 0 duplicate rows in meteo_df after duplicate cleaning\n",
      "2025-03-08 07:00:45,601 - __main__ - INFO - Unreachable leads deleted\n",
      "2025-03-08 07:00:45,604 - __main__ - INFO - outliers_df dataframe created\n",
      "2025-03-08 07:00:45,604 - __main__ - INFO - Number of outliers: 1096\n",
      "2025-03-08 07:00:45,606 - __main__ - INFO - outliers removed from sales_phases_funnel_df\n",
      "2025-03-08 07:00:45,606 - __main__ - INFO - outliers_df dataframe added to list_of_dfs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_of_transformed_dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 308\u001b[0m\n\u001b[1;32m    303\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    304\u001b[0m     connection\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    307\u001b[0m dfs_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzipcode_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mlist_of_transformed_dfs\u001b[49m[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: list_of_transformed_dfs[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_fact\u001b[39m\u001b[38;5;124m\"\u001b[39m: list_of_transformed_dfs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    311\u001b[0m     }\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_to_database\u001b[39m(dfs_dict, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    315\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    Write a dataframe into a MySql table.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m        if_exists (str): Default 'append'\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_transformed_dfs' is not defined"
     ]
    }
   ],
   "source": [
    "#-----------FOR THE BOIS----------------------\n",
    "\n",
    "#--------LIBRARIES FOR THE BOIS (THE ONES I USED, THE USUAL ONES)-----------\n",
    "\n",
    "#COOL LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tools.logs import log_wrap\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Create logger instance\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SET PATHS OF 3 COOL CSVs\n",
    "FILENAME_sales_phases_funnel_df = os.path.join(os.getcwd(), r'data/sale_phases_funnel.csv')\n",
    "FILENAME_zipcode_df = os.path.join(os.getcwd(), r'data/zipcode_eae.csv')\n",
    "FILENAME_meteo_df = os.path.join(os.getcwd(), r'data/meteo_eae.csv')\n",
    "\n",
    "\n",
    "#SETTING TYPES\n",
    "#Sales\n",
    "SALES_TYPES = {'LEAD_ID':'str','FINANCING_TYPE':'str',\n",
    "                    'CURRENT_PHASE':'str','PHASE_PRE_KO':'str',\n",
    "                    'IS_MODIFIED':'bool','ZIPCODE':'str', \n",
    "                    'VISITING_COMPANY': 'str', 'KO_REASON': 'str', \n",
    "                    'INSTALLATION_PEAK_POWER_KW': 'float64', \n",
    "                    'INSTALLATION_PRICE': 'float', \n",
    "                    'N_PANELS': 'int', 'CUSOMER_TYPE': 'str' }\n",
    "\n",
    "#Zipcosdes\n",
    "ZIPCODE_TYPES = {'ZIPCODE':'str','ZC_LATITUDE':'float64',\n",
    "                    'ZC_LONGITUDE':'float64','AUTONOMOUS_COMMUNITY':'str',\n",
    "                    'AUTONOMOUS_COMMUNITY_NK':'str','PROVINCE':'str'}\n",
    "\n",
    "#Meteo\n",
    "METEO_TYPES = {'temperature': 'float', 'relative_humidity': 'float', \n",
    "            'precipitation_rate': 'float', 'wind_speed': 'float', \n",
    "            'zipcode': 'str' \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------FUNCTIONS FOR THE BOIS-----------\n",
    "\n",
    "#CREATE 3 COOL DATAFRAMES FUNCTION\n",
    "#Creates 3 super cool dataframes from the CSVs with the data types set from the start.\n",
    "def dataFrameCreate():\n",
    "\n",
    "    #SALES FUNNEL DATAFRAME\n",
    "\n",
    "    #Dictionary with data types\n",
    "   \n",
    "\n",
    "    #Reading CSV to create dataframe with datatypes implemented from dictionary and additional date time datatypes.\n",
    "    sales_phases_funnel_df = pd.read_csv(\n",
    "        FILENAME_sales_phases_funnel_df, \n",
    "        delimiter=';', \n",
    "        dtype=SALES_TYPES,\n",
    "        parse_dates=['OFFER_SENT_DATE', 'CONTRACT_1_DISPATCH_DATE', \n",
    "                    'CONTRACT_2_DISPATCH_DATE', \n",
    "                    'CONTRACT_1_SIGNATURE_DATE', \n",
    "                    'CONTRACT_2_SIGNATURE_DATE',\n",
    "                    'VISIT_DATE',\n",
    "                    'TECHNICAL_REVIEW_DATE',\n",
    "                    'PROJECT_VALIDATION_DATE',\n",
    "                    'SALE_DISMISSAL_DATE',\n",
    "                    'KO_DATE'],\n",
    "                    \n",
    "        dayfirst=True  # This replaces the dayfirst=True in your to_datetime call\n",
    "    )\n",
    "\n",
    "    logger.info('sales_phases_funnel_df created')\n",
    "\n",
    "\n",
    "\n",
    "    #ZIPCODE DATAFRAME\n",
    "\n",
    "    # Reading CSV to create dataframe with datatypes implemented from dictionary\n",
    "    zipcode_df = pd.read_csv(FILENAME_zipcode_df, delimiter=',', dtype=ZIPCODE_TYPES)\n",
    "\n",
    "\n",
    "    logger.info('zipcodedf created')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #METEO DATAFRAME\n",
    "\n",
    "    # Reading CSV to create dataframe with datatypes implemented from dictionary and \n",
    "    # additional date time datatype formatted to match the ones from the sales dataframe.\n",
    "    meteo_df = pd.read_csv(FILENAME_meteo_df, delimiter=';',\n",
    "        dtype=METEO_TYPES, parse_dates=['date'],  # Replace with actual column name\n",
    "        date_format='%Y/%m/%d %H:%M:%S.%f'  # This matches your input format\n",
    "    )\n",
    "\n",
    "    logger.info('meteo_df created')\n",
    "    list_of_dfs = [sales_phases_funnel_df, zipcode_df, meteo_df]\n",
    "\n",
    "    return list_of_dfs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "list_of_dfs = dataFrameCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--GLOBAL CLEANING FUNCTION--\n",
    "\n",
    "\n",
    "#DROPPING DUPLICATES FOR ALL DATAFRAMES\n",
    "\n",
    "#creating the drop duplicate function\n",
    "def dropDupli(dfs):\n",
    "    #log\n",
    "    logger.info(f'There are {dfs[0].duplicated().sum()} duplicate rows in sales_funnel_df before duplicate cleaning') \n",
    "    logger.info(f'There are {dfs[1].duplicated().sum()} duplicate rows in zipcode_df before duplicate cleaning')\n",
    "    logger.info(f'There are {dfs[2].duplicated().sum()} duplicate rows in meteo_df before duplicate cleaning')  \n",
    "    #DroppingDupli\n",
    "    dfs[0].drop_duplicates(inplace=True)\n",
    "    dfs[1].drop_duplicates(inplace=True)\n",
    "    dfs[2].drop_duplicates(inplace=True)\n",
    "    # Log after\n",
    "    logger.info(f'There are {dfs[0].duplicated().sum()} duplicate rows in sales_funnel_df after duplicate cleaning') \n",
    "    logger.info(f'There are {dfs[1].duplicated().sum()} duplicate rows in zipcode_df after duplicate cleaning')\n",
    "    logger.info(f'There are {dfs[2].duplicated().sum()} duplicate rows in meteo_df after duplicate cleaning')\n",
    "\n",
    "    return dfs  # Return dfs instead of undefined variables\n",
    "\n",
    "\n",
    "\n",
    "list_of_dfs = dropDupli(list_of_dfs)\n",
    "\n",
    "\n",
    "\n",
    "# --SALES FUNNEL DATAFRAME CLEANING FUCTIONS--\n",
    "\n",
    "\n",
    "#DELETE UNUSABLE LEADS FUNCTION \n",
    "\n",
    "# Drop rows where KO_REASON is \"Unreachable\"\n",
    "def delete_unreachable_leads(dfs):\n",
    "    dfs[0] = dfs[0][~((dfs[0]['CURRENT_PHASE'] == 'KO') & (dfs[0]['KO_REASON'] == 'Unreachable'))]\n",
    "    # Reset the index of the updated DataFrame\n",
    "    dfs[0].reset_index(drop=True, inplace=True)\n",
    "    logger.info('Unreachable leads deleted')\n",
    "    return dfs\n",
    "\n",
    "list_of_dfs = delete_unreachable_leads(list_of_dfs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# REMOVE OUTLIERS FUNCTION\n",
    "\n",
    "\n",
    "def delete_outliers(dfs):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = dfs[0]['INSTALLATION_PRICE'].quantile(0.25)\n",
    "    Q3 = dfs[0]['INSTALLATION_PRICE'].quantile(0.75)\n",
    "    \n",
    "\n",
    "    # Calculate the Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Identify outliers inside a new Data Frame\n",
    "    outliers_df = dfs[0][(dfs[0]['INSTALLATION_PRICE'] < (Q1 - 1.5 * IQR)) | \n",
    "                                     (dfs[0]['INSTALLATION_PRICE'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "    # Print the number of outliers\n",
    "    logger.info('outliers_df dataframe created')\n",
    "    logger.info(f'Number of outliers: {len(outliers_df)}')\n",
    "    \n",
    "    # Update sales_phases_funnel_df to exclude the outliers\n",
    "    sales_phases_funnel_df = dfs[0][~((dfs[0]['INSTALLATION_PRICE'] < (Q1 - 1.5 * IQR)) | \n",
    "                                                  (dfs[0]['INSTALLATION_PRICE'] > (Q3 + 1.5 * IQR)))]\n",
    "    logger.info('outliers removed from sales_phases_funnel_df')\n",
    "    dfs.append(outliers_df)\n",
    "    logger.info(f'outliers_df dataframe added to list_of_dfs')\n",
    "    return dfs\n",
    "    \n",
    "\n",
    "\n",
    "list_of_dfs = delete_outliers(list_of_dfs)\n",
    "\n",
    "\n",
    "#This is how you would use it in th final etl:\n",
    "'''\n",
    "from tools.cleaning import dataFrameCreate, dropDupli, delete_unreachable_leads, delete_outliers\n",
    "\n",
    "\n",
    "\n",
    "list_of_dfs = dataFrameCreate()\n",
    "list_of_dfs = dropDupli(list_of_dfs)\n",
    "list_of_dfs = delete_unreachable_leads(list_of_dfs) #this one we may exclude\n",
    "list_of_dfs = delete_outliers(list_of_dfs)\n",
    "\n",
    "print(list_of_dfs)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@log_wrap\n",
    "def transform_data(data: list, logger) -> list:\n",
    "    '''\n",
    "    Takes a list of dfs as arguments of size 3 and returns a list of transformed dataframes\n",
    "    Order:\n",
    "    [0] = Sales\n",
    "    [1] = Zipcode\n",
    "    [2] = Weather\n",
    "    '''\n",
    "    try:\n",
    "        logger.info('Reading Dataframes...')\n",
    "        sales_fact_df_raw = data[0]\n",
    "        logger.info(f'Sales Data has {len(sales_fact_df_raw)} records.')\n",
    "        zipcode_dim_df_raw = data[1]\n",
    "        logger.info(f'Zipcode Data has {len(zipcode_dim_df_raw)} records.')\n",
    "        weather_dim_df_raw = data[2]\n",
    "        logger.info(f'Weather Data has {len(weather_dim_df_raw)} records.')\n",
    "        \n",
    "        logger.info(f'Processing Transformations...')\n",
    "        sales_fact_df_raw.columns = sales_fact_df_raw.columns.str.lower()\n",
    "        zipcode_dim_df_raw.columns = zipcode_dim_df_raw.columns.str.lower()\n",
    "        weather_dim_df_raw.columns = weather_dim_df_raw.columns.str.lower()\n",
    "        \n",
    "        logger.info(f'Creating a PK in zipcode_dim_df_raw...')\n",
    "        zipcode_dim_df_raw.insert(0,'zipcode_id',range(1, len(zipcode_dim_df_raw) + 1))\n",
    "        zipcode_dim_df_raw['zipcode_id'] = zipcode_dim_df_raw['zipcode_id'].astype('int32')\n",
    "        zipcode_dim_df = zipcode_dim_df_raw\n",
    "        \n",
    "        logger.info(f'Grouping weather_dim_df_raw...')\n",
    "        weather_dim_df_raw['date'] = weather_dim_df_raw['date'].dt.year\n",
    "        weather_dim_df_raw = weather_dim_df_raw.groupby(['date','zipcode']).mean().reset_index()\n",
    "        \n",
    "        logger.info(f'Adding FK zipcode_id in weather table...')\n",
    "        weather_dim_df = pd.merge(weather_dim_df_raw,zipcode_dim_df_raw,on= 'zipcode', how='left')\n",
    "        \n",
    "        logger.info(f'Dropping null zipcode_id from weather table...')\n",
    "        weather_dim_df = weather_dim_df.dropna()\n",
    "        \n",
    "        logger.info(f'Creating a PK in weather_dim_df_raw...')\n",
    "        weather_dim_df.insert(0,'weather_id',range(1, len(weather_dim_df) + 1))\n",
    "        weather_dim_df['weather_id'] = weather_dim_df['weather_id'].astype('int32')\n",
    "        \n",
    "        logger.info(f'Creating a PK in sales_fact_df_raw...')\n",
    "        sales_fact_df_raw.insert(0,'sales_id',range(1, len(sales_fact_df_raw) + 1))\n",
    "        sales_fact_df_raw['sales_id'] = sales_fact_df_raw['sales_id'].astype('int32')\n",
    "        \n",
    "        logger.info(f'Adding calculated column most_recent_contract_signature to sales_fact_df...')\n",
    "        sales_fact_df_raw.insert(16,'most_recent_contract_signature', \\\n",
    "            sales_fact_df_raw[['contract_1_signature_date', 'contract_2_signature_date']].max(axis=1))\n",
    "\n",
    "        logger.info(f'Adding FK zipcode_id in sales table...')\n",
    "        sales_fact_df = pd.merge(sales_fact_df_raw, zipcode_dim_df_raw, on='zipcode', how='left')\n",
    "        \n",
    "        logger.info(f'Handling column types, names and selection...')\n",
    "        weather_dim_df = weather_dim_df.rename(columns=FINAL_NAMES_WEATHER)\n",
    "        weather_dim_df = weather_dim_df[FINAL_COLS_WEATHER]\n",
    "        sales_fact_df = sales_fact_df[FINAL_COLS_SALES]        \n",
    "        \n",
    "        logger.info(f'Packing data for loading...')\n",
    "        list_of_transformed_dfs = [zipcode_dim_df,weather_dim_df, sales_fact_df]\n",
    "        \n",
    "        return list_of_transformed_dfs\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f'Transformation error: {e}', exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def create_table():\n",
    "    connection = mysql.connector.connect(\n",
    "        user = creds['mysql-db']['username'],\n",
    "        password = creds['mysql-db']['password'],\n",
    "        host = creds['mysql-db']['host'],\n",
    "        database = creds['mysql-db']['database'],\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    cursor.execute(QUERY_CREATE_ZIPCODE_DIM_TABLE)\n",
    "    cursor.execute(QUERY_CREATE_WEATHER_DIM_TABLE)\n",
    "    cursor.execute(QUERY_CREATE_SALES_FT_TABLE)\n",
    "    connection.commit()\n",
    "    print(\"Table structures created successfully.\")\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "dfs_dict = {\n",
    "        \"zipcode_dim\": list_of_transformed_dfs[0],\n",
    "        \"weather_dim\": list_of_transformed_dfs[1],\n",
    "        \"sales_fact\": list_of_transformed_dfs[2]\n",
    "    }\n",
    "\n",
    "\n",
    "def write_to_database(dfs_dict, if_exists='append'):\n",
    "    \"\"\"\n",
    "    Write a dataframe into a MySql table.\n",
    "\n",
    "    Args:\n",
    "        dfs_dict: The list of tables to load to along with the dfs to insert\n",
    "        if_exists (str): Default 'append'\n",
    "    \"\"\"\n",
    "\n",
    "    _db_user = creds['username']\n",
    "    _db_password = creds['password']\n",
    "    _db_host = creds['host']\n",
    "    _db_name = creds['database']\n",
    "    engine = create_engine(f\"mysql+pymysql://{_db_user}:{_db_password}@{_db_host}:3306/{_db_name}\")\n",
    "    with engine.connect() as connection:\n",
    "        for table_name, df in dfs_dict.items():\n",
    "            if isinstance(df, pd.DataFrame): \n",
    "                df.to_sql(table_name, con=connection, if_exists=if_exists, index=False)\n",
    "                print(f\"Data successfully inserted into {table_name}\")\n",
    "            else:\n",
    "                print(f\"Skipping {table_name}: Not a valid DataFrame\")\n",
    "    \n",
    "    # return logger.info(\"Completed uploading all data..\")\n",
    "\n",
    "\n",
    "create_table()\n",
    "write_to_database(dfs_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
