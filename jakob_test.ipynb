{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries\n",
    "import os\n",
    "\n",
    "# Importing third party libraries\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "# Importing custom libraries\n",
    "from tools.sql_tools import write_to_database\n",
    "from tools.logs import log_wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Column Names\n",
    "FINAL_NAMES_WEATHER = {'date':'year','temperature': 'avg_temperature', 'relative_humidity': 'avg_relative_humidity',\n",
    "                                               'precipitation_rate':'avg_precipitation_rate','wind_speed':'avg_wind_speed'}\n",
    "\n",
    "# Final Columns\n",
    "FINAL_COLS_SALES = ['sales_id','zipcode_id','lead_id','financing_type','current_phase','phase_pre_ko',\n",
    "              'is_modified','offer_sent_date','contract_1_dispatch_date','contract_2_dispatch_date','contract_1_signature_date',\n",
    "              'contract_2_signature_date','most_recent_contract_signature','visit_date','technical_review_date',\n",
    "              'project_validation_date','sale_dismissal_date','ko_date','visiting_company','ko_reason',\n",
    "              'installation_peak_power_kw','installation_price','n_panels','cusomer_type']\n",
    "FINAL_COLS_WEATHER=['weather_id','zipcode_id','year','avg_temperature','avg_relative_humidity','avg_precipitation_rate',\n",
    "                    'avg_wind_speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries\n",
    "import os\n",
    "\n",
    "# Importing third party libraries\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(os.getcwd(), 'creds.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILENAME_creds, \"r\") as file:\n",
    "    creds = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_CREATE_ZIPCODE_DIM_TABLE ='''\n",
    "CREATE TABLE IF NOT EXISTS zipcode_dim (\n",
    "  zipcode_id int NOT NULL,\n",
    "  zipcode varchar(50) NOT NULL,\n",
    "  zc_latitude float,\n",
    "  zc_longitude float,\n",
    "  autonomous_community varchar(50),\n",
    "  autonomous_community_nk varchar(50),\n",
    "  province varchar(50)\n",
    ");\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_CREATE_WEATHER_DIM_TABLE = '''\n",
    "CREATE TABLE IF NOT EXISTS weather_dim (\n",
    "  weather_id int NOT NULL,\n",
    "  zipcode_id int NOT NULL,\n",
    "  year int NOT NULL,\n",
    "  avg_temperature float NOT NULL,\n",
    "  avg_relative_humidity float NOT NULL,\n",
    "  avg_precipitation_rate float NOT NULL,\n",
    "  avg_wind_speed float NOT NULL\n",
    ");\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_CREATE_SALES_FT_TABLE = '''\n",
    "CREATE TABLE IF NOT EXISTS sales_fact (\n",
    "  sales_id int NOT NULL,\n",
    "  zipcode_id int,\n",
    "  lead_id varchar(50),\n",
    "  financing_type varchar(50) NOT NULL,\n",
    "  current_phase varchar(50) NOT NULL,\n",
    "  phase_pre_ko varchar(50) NOT NULL,\n",
    "  is_modified int,\n",
    "  offer_sent_date date NOT NULL,\n",
    "  contract_1_dispatch_date date NOT NULL,\n",
    "  contract_2_dispatch_date date NOT NULL,\n",
    "  contract_1_signature_date date NOT NULL,\n",
    "  contract_2_signature_date date NOT NULL,\n",
    "  most_recent_contract_signature date NOT NULL,\n",
    "  visit_date date NOT NULL,\n",
    "  technical_review_date date NOT NULL,\n",
    "  project_validation_date date NOT NULL,\n",
    "  sale_dismissal_date date NOT NULL,\n",
    "  ko_date date NOT NULL,\n",
    "  visiting_company varchar(50) NOT NULL,\n",
    "  ko_reason varchar(50) NOT NULL,\n",
    "  installation_peak_power_kw float NOT NULL,\n",
    "  installation_price float NOT NULL,\n",
    "  n_panels smallint,\n",
    "  cusomer_type varchar(50) NOT NULL\n",
    ");\n",
    "'''\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    connection = mysql.connector.connect(\n",
    "        user = creds['mysql-db']['username'],\n",
    "        password = creds['mysql-db']['password'],\n",
    "        host = creds['mysql-db']['host'],\n",
    "        database = creds['mysql-db']['database'],\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    cursor.execute(QUERY_CREATE_ZIPCODE_DIM_TABLE)\n",
    "    cursor.execute(QUERY_CREATE_WEATHER_DIM_TABLE)\n",
    "    cursor.execute(QUERY_CREATE_SALES_FT_TABLE)\n",
    "    connection.commit()\n",
    "    print(\"Table structures created successfully.\")\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_database(dfs_dict, if_exists='append'):\n",
    "    \"\"\"\n",
    "    Write a dataframe into a MySql table.\n",
    "\n",
    "    Args:\n",
    "        dfs_dict: The list of tables to load to along with the dfs to insert\n",
    "        if_exists (str): Default 'append'\n",
    "    \"\"\"\n",
    "\n",
    "    _db_user = creds['username']\n",
    "    _db_password = creds['password']\n",
    "    _db_host = creds['host']\n",
    "    _db_name = creds['database']\n",
    "    engine = create_engine(f\"mysql+pymysql://{_db_user}:{_db_password}@{_db_host}:3306/{_db_name}\")\n",
    "    with engine.connect() as connection:\n",
    "        for table_name, df in dfs_dict.items():\n",
    "            if isinstance(df, pd.DataFrame): \n",
    "                df.to_sql(table_name, con=connection, if_exists=if_exists, index=False)\n",
    "                print(f\"Data successfully inserted into {table_name}\")\n",
    "            else:\n",
    "                print(f\"Skipping {table_name}: Not a valid DataFrame\")\n",
    "    \n",
    "    # return logger.info(\"Completed uploading all data..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table structures created successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dfs_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     create_table()\n\u001b[0;32m----> 3\u001b[0m     write_to_database(\u001b[43mdfs_dict\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs_dict' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    create_table()\n",
    "    write_to_database(dfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 16:37:59,707 - __main__ - INFO - sales_phases_funnel_df created\n",
      "2025-03-08 16:37:59,716 - __main__ - INFO - zipcodedf created\n",
      "2025-03-08 16:38:01,836 - __main__ - INFO - meteo_df created\n",
      "2025-03-08 16:38:01,945 - __main__ - INFO - There are 0 duplicate rows in sales_funnel_df before duplicate cleaning\n",
      "2025-03-08 16:38:01,952 - __main__ - INFO - There are 0 duplicate rows in zipcode_df before duplicate cleaning\n",
      "2025-03-08 16:38:03,100 - __main__ - INFO - There are 0 duplicate rows in meteo_df before duplicate cleaning\n",
      "2025-03-08 16:38:04,196 - __main__ - INFO - There are 0 duplicate rows in sales_funnel_df after duplicate cleaning\n",
      "2025-03-08 16:38:04,199 - __main__ - INFO - There are 0 duplicate rows in zipcode_df after duplicate cleaning\n",
      "2025-03-08 16:38:05,516 - __main__ - INFO - There are 0 duplicate rows in meteo_df after duplicate cleaning\n",
      "2025-03-08 16:38:05,548 - __main__ - INFO - Unreachable leads deleted\n",
      "2025-03-08 16:38:05,556 - __main__ - INFO - outliers_df dataframe created\n",
      "2025-03-08 16:38:05,556 - __main__ - INFO - Number of outliers: 1096\n",
      "2025-03-08 16:38:05,559 - __main__ - INFO - outliers removed from sales_phases_funnel_df\n",
      "2025-03-08 16:38:05,560 - __main__ - INFO - outliers_df dataframe added to list_of_dfs\n",
      "2025-03-08 16:38:05,562 - transform_data - INFO - Executing function: transform_data\n",
      "2025-03-08 16:38:05,563 - transform_data - INFO - Reading Dataframes...\n",
      "2025-03-08 16:38:05,564 - transform_data - INFO - Sales Data has 28864 records.\n",
      "2025-03-08 16:38:05,565 - transform_data - INFO - Zipcode Data has 11407 records.\n",
      "2025-03-08 16:38:05,565 - transform_data - INFO - Weather Data has 4114206 records.\n",
      "2025-03-08 16:38:05,565 - transform_data - INFO - Processing Transformations...\n",
      "2025-03-08 16:38:05,567 - transform_data - INFO - Creating a PK in zipcode_dim_df_raw...\n",
      "2025-03-08 16:38:05,569 - transform_data - INFO - Grouping weather_dim_df_raw...\n",
      "2025-03-08 16:38:05,980 - transform_data - INFO - Adding FK zipcode_id in weather table...\n",
      "2025-03-08 16:38:05,989 - transform_data - INFO - Dropping null zipcode_id from weather table...\n",
      "2025-03-08 16:38:05,994 - transform_data - INFO - Creating a PK in weather_dim_df_raw...\n",
      "2025-03-08 16:38:05,995 - transform_data - INFO - Creating a PK in sales_fact_df_raw...\n",
      "2025-03-08 16:38:05,996 - transform_data - INFO - Adding calculated column most_recent_contract_signature to sales_fact_df...\n",
      "2025-03-08 16:38:06,001 - transform_data - INFO - Adding FK zipcode_id in sales table...\n",
      "2025-03-08 16:38:06,009 - transform_data - INFO - Handling column types, names and selection...\n",
      "2025-03-08 16:38:06,016 - transform_data - INFO - Packing data for loading...\n",
      "2025-03-08 16:38:06,017 - transform_data - INFO - Completed function: transform_data\n",
      "2025-03-08 16:38:06,025 - create_table - INFO - Executing function: create_table\n",
      "2025-03-08 16:38:06,145 - create_table - INFO - Table structures created successfully.\n",
      "2025-03-08 16:38:06,146 - create_table - INFO - Completed function: create_table\n",
      "2025-03-08 16:38:06,146 - write_to_database - INFO - Executing function: write_to_database\n",
      "2025-03-08 16:38:06,599 - write_to_database - INFO - Data successfully inserted into zipcode_dim\n",
      "2025-03-08 16:38:06,786 - write_to_database - INFO - Data successfully inserted into weather_dim\n",
      "2025-03-08 16:38:07,963 - write_to_database - INFO - Data successfully inserted into sales_fact\n",
      "2025-03-08 16:38:07,965 - write_to_database - INFO - Completed function: write_to_database\n",
      "2025-03-08 16:38:07,966 - create_relationships - INFO - Executing function: create_relationships\n",
      "2025-03-08 16:38:08,505 - create_relationships - INFO - ALTER TABLE query executed successfully.\n",
      "2025-03-08 16:38:08,506 - create_relationships - INFO - Completed function: create_relationships\n"
     ]
    }
   ],
   "source": [
    "#-----------FOR THE BOIS----------------------\n",
    "\n",
    "#--------LIBRARIES FOR THE BOIS (THE ONES I USED, THE USUAL ONES)-----------\n",
    "\n",
    "#COOL LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tools.logs import log_wrap\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Create logger instance\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "# Importing SQL Queries\n",
    "SQL_DIR = os.path.join(os.getcwd(),'SQL')\n",
    "FILENAME_CREATE_ZIPCODE_DIM = os.path.join(SQL_DIR, r'CREATE_ZIPCODE_DIM')\n",
    "with open(FILENAME_CREATE_ZIPCODE_DIM, 'r') as CREATE_ZIPCODE_DIM_FILE:\n",
    "    CREATE_ZIPCODE_DIM = CREATE_ZIPCODE_DIM_FILE.read()\n",
    "\n",
    "FILENAME_CREATE_WEATHER_DIM = os.path.join(SQL_DIR, r'CREATE_WEATHER_DIM')\n",
    "with open(FILENAME_CREATE_WEATHER_DIM, 'r') as CREATE_WEATHER_DIM_FILE:\n",
    "    CREATE_WEATHER_DIM = CREATE_WEATHER_DIM_FILE.read()\n",
    "\n",
    "FILENAME_CREATE_SALES_FACT = os.path.join(SQL_DIR, r'CREATE_SALES_FACT')\n",
    "with open(FILENAME_CREATE_SALES_FACT, 'r') as CREATE_SALES_FACT_FILE:\n",
    "    CREATE_SALES_FACT = CREATE_SALES_FACT_FILE.read()\n",
    "\n",
    "FILENAME_ALTER_ZIPCODE_DIM = os.path.join(SQL_DIR, r'ALTER_ZIPCODE_DIM')\n",
    "with open(FILENAME_ALTER_ZIPCODE_DIM, 'r') as ALTER_ZIPCODE_DIM_FILE:\n",
    "    ALTER_ZIPCODE_DIM = ALTER_ZIPCODE_DIM_FILE.read()\n",
    "\n",
    "FILENAME_ALTER_WEATHER_DIM = os.path.join(SQL_DIR, r'ALTER_WEATHER_DIM')\n",
    "with open(FILENAME_ALTER_WEATHER_DIM, 'r') as ALTER_WEATHER_DIM_FILE:\n",
    "    ALTER_WEATHER_DIM = ALTER_WEATHER_DIM_FILE.read()\n",
    "\n",
    "FILENAME_ALTER_SALES_FACT_1 = os.path.join(SQL_DIR, r'ALTER_SALES_FACT_1')\n",
    "with open(FILENAME_ALTER_SALES_FACT_1, 'r') as ALTER_SALES_FACT_FILE_1:\n",
    "    ALTER_SALES_FACT_1 = ALTER_SALES_FACT_FILE_1.read()\n",
    "\n",
    "FILENAME_ALTER_SALES_FACT_2 = os.path.join(SQL_DIR, r'ALTER_SALES_FACT_2')\n",
    "with open(FILENAME_ALTER_SALES_FACT_2, 'r') as ALTER_SALES_FACT_FILE_2:\n",
    "    ALTER_SALES_FACT_2 = ALTER_SALES_FACT_FILE_2.read()\n",
    "\n",
    "# SET PATHS OF 3 COOL CSVs\n",
    "DATA_DIR = os.path.join(os.getcwd(),'data')\n",
    "FILENAME_sales_phases_funnel_df = os.path.join(DATA_DIR, r'sale_phases_funnel.csv')\n",
    "FILENAME_zipcode_df = os.path.join(DATA_DIR, r'zipcode_eae.csv')\n",
    "FILENAME_meteo_df = os.path.join(DATA_DIR, r'meteo_eae.csv')\n",
    "\n",
    "#SETTING TYPES\n",
    "#Sales\n",
    "SALES_TYPES = {'LEAD_ID':'str','FINANCING_TYPE':'str',\n",
    "                    'CURRENT_PHASE':'str','PHASE_PRE_KO':'str',\n",
    "                    'IS_MODIFIED':'bool','ZIPCODE':'str', \n",
    "                    'VISITING_COMPANY': 'str', 'KO_REASON': 'str', \n",
    "                    'INSTALLATION_PEAK_POWER_KW': 'float64', \n",
    "                    'INSTALLATION_PRICE': 'float', \n",
    "                    'N_PANELS': 'int', 'CUSOMER_TYPE': 'str' }\n",
    "\n",
    "#Zipcosdes\n",
    "ZIPCODE_TYPES = {'ZIPCODE':'str','ZC_LATITUDE':'float64',\n",
    "                    'ZC_LONGITUDE':'float64','AUTONOMOUS_COMMUNITY':'str',\n",
    "                    'AUTONOMOUS_COMMUNITY_NK':'str','PROVINCE':'str'}\n",
    "\n",
    "#Meteo\n",
    "METEO_TYPES = {'temperature': 'float', 'relative_humidity': 'float', \n",
    "            'precipitation_rate': 'float', 'wind_speed': 'float', \n",
    "            'zipcode': 'str' \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------FUNCTIONS FOR THE BOIS-----------\n",
    "\n",
    "#CREATE 3 COOL DATAFRAMES FUNCTION\n",
    "#Creates 3 super cool dataframes from the CSVs with the data types set from the start.\n",
    "def dataFrameCreate():\n",
    "\n",
    "    #SALES FUNNEL DATAFRAME\n",
    "\n",
    "    #Dictionary with data types\n",
    "   \n",
    "\n",
    "    #Reading CSV to create dataframe with datatypes implemented from dictionary and additional date time datatypes.\n",
    "    sales_phases_funnel_df = pd.read_csv(\n",
    "        FILENAME_sales_phases_funnel_df, \n",
    "        delimiter=';', \n",
    "        dtype=SALES_TYPES,\n",
    "        parse_dates=['OFFER_SENT_DATE', 'CONTRACT_1_DISPATCH_DATE', \n",
    "                    'CONTRACT_2_DISPATCH_DATE', \n",
    "                    'CONTRACT_1_SIGNATURE_DATE', \n",
    "                    'CONTRACT_2_SIGNATURE_DATE',\n",
    "                    'VISIT_DATE',\n",
    "                    'TECHNICAL_REVIEW_DATE',\n",
    "                    'PROJECT_VALIDATION_DATE',\n",
    "                    'SALE_DISMISSAL_DATE',\n",
    "                    'KO_DATE'],\n",
    "                    \n",
    "        dayfirst=True  # This replaces the dayfirst=True in your to_datetime call\n",
    "    )\n",
    "\n",
    "    logger.info('sales_phases_funnel_df created')\n",
    "\n",
    "\n",
    "\n",
    "    #ZIPCODE DATAFRAME\n",
    "\n",
    "    # Reading CSV to create dataframe with datatypes implemented from dictionary\n",
    "    zipcode_df = pd.read_csv(FILENAME_zipcode_df, delimiter=',', dtype=ZIPCODE_TYPES)\n",
    "\n",
    "\n",
    "    logger.info('zipcodedf created')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #METEO DATAFRAME\n",
    "\n",
    "    # Reading CSV to create dataframe with datatypes implemented from dictionary and \n",
    "    # additional date time datatype formatted to match the ones from the sales dataframe.\n",
    "    meteo_df = pd.read_csv(FILENAME_meteo_df, delimiter=';',\n",
    "        dtype=METEO_TYPES, parse_dates=['date'],  # Replace with actual column name\n",
    "        date_format='%Y/%m/%d %H:%M:%S.%f'  # This matches your input format\n",
    "    )\n",
    "\n",
    "    logger.info('meteo_df created')\n",
    "    list_of_dfs = [sales_phases_funnel_df, zipcode_df, meteo_df]\n",
    "\n",
    "    return list_of_dfs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "list_of_dfs = dataFrameCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--GLOBAL CLEANING FUNCTION--\n",
    "\n",
    "\n",
    "#DROPPING DUPLICATES FOR ALL DATAFRAMES\n",
    "\n",
    "#creating the drop duplicate function\n",
    "def dropDupli(dfs):\n",
    "    #log\n",
    "    logger.info(f'There are {dfs[0].duplicated().sum()} duplicate rows in sales_funnel_df before duplicate cleaning') \n",
    "    logger.info(f'There are {dfs[1].duplicated().sum()} duplicate rows in zipcode_df before duplicate cleaning')\n",
    "    logger.info(f'There are {dfs[2].duplicated().sum()} duplicate rows in meteo_df before duplicate cleaning')  \n",
    "    #DroppingDupli\n",
    "    dfs[0].drop_duplicates(inplace=True)\n",
    "    dfs[1].drop_duplicates(inplace=True)\n",
    "    dfs[2].drop_duplicates(inplace=True)\n",
    "    # Log after\n",
    "    logger.info(f'There are {dfs[0].duplicated().sum()} duplicate rows in sales_funnel_df after duplicate cleaning') \n",
    "    logger.info(f'There are {dfs[1].duplicated().sum()} duplicate rows in zipcode_df after duplicate cleaning')\n",
    "    logger.info(f'There are {dfs[2].duplicated().sum()} duplicate rows in meteo_df after duplicate cleaning')\n",
    "\n",
    "    return dfs  # Return dfs instead of undefined variables\n",
    "\n",
    "\n",
    "\n",
    "list_of_dfs = dropDupli(list_of_dfs)\n",
    "\n",
    "\n",
    "\n",
    "# --SALES FUNNEL DATAFRAME CLEANING FUCTIONS--\n",
    "\n",
    "\n",
    "#DELETE UNUSABLE LEADS FUNCTION \n",
    "\n",
    "# Drop rows where KO_REASON is \"Unreachable\"\n",
    "def delete_unreachable_leads(dfs):\n",
    "    dfs[0] = dfs[0][~((dfs[0]['CURRENT_PHASE'] == 'KO') & (dfs[0]['KO_REASON'] == 'Unreachable'))]\n",
    "    # Reset the index of the updated DataFrame\n",
    "    dfs[0].reset_index(drop=True, inplace=True)\n",
    "    logger.info('Unreachable leads deleted')\n",
    "    return dfs\n",
    "\n",
    "list_of_dfs = delete_unreachable_leads(list_of_dfs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# REMOVE OUTLIERS FUNCTION\n",
    "\n",
    "\n",
    "def delete_outliers(dfs):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = dfs[0]['INSTALLATION_PRICE'].quantile(0.25)\n",
    "    Q3 = dfs[0]['INSTALLATION_PRICE'].quantile(0.75)\n",
    "    \n",
    "\n",
    "    # Calculate the Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Identify outliers inside a new Data Frame\n",
    "    outliers_df = dfs[0][(dfs[0]['INSTALLATION_PRICE'] < (Q1 - 1.5 * IQR)) | \n",
    "                                     (dfs[0]['INSTALLATION_PRICE'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "    # Print the number of outliers\n",
    "    logger.info('outliers_df dataframe created')\n",
    "    logger.info(f'Number of outliers: {len(outliers_df)}')\n",
    "    \n",
    "    # Update sales_phases_funnel_df to exclude the outliers\n",
    "    sales_phases_funnel_df = dfs[0][~((dfs[0]['INSTALLATION_PRICE'] < (Q1 - 1.5 * IQR)) | \n",
    "                                                  (dfs[0]['INSTALLATION_PRICE'] > (Q3 + 1.5 * IQR)))]\n",
    "    logger.info('outliers removed from sales_phases_funnel_df')\n",
    "    dfs.append(outliers_df)\n",
    "    logger.info(f'outliers_df dataframe added to list_of_dfs')\n",
    "    return dfs\n",
    "    \n",
    "\n",
    "\n",
    "list_of_dfs = delete_outliers(list_of_dfs)\n",
    "\n",
    "\n",
    "#This is how you would use it in th final etl:\n",
    "'''\n",
    "from tools.cleaning import dataFrameCreate, dropDupli, delete_unreachable_leads, delete_outliers\n",
    "\n",
    "\n",
    "\n",
    "list_of_dfs = dataFrameCreate()\n",
    "list_of_dfs = dropDupli(list_of_dfs)\n",
    "list_of_dfs = delete_unreachable_leads(list_of_dfs) #this one we may exclude\n",
    "list_of_dfs = delete_outliers(list_of_dfs)\n",
    "\n",
    "print(list_of_dfs)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@log_wrap\n",
    "def transform_data(data: list, logger) -> list:\n",
    "    '''\n",
    "    Takes a list of dfs as arguments of size 3 and returns a list of transformed dataframes\n",
    "    Order:\n",
    "    [0] = Sales\n",
    "    [1] = Zipcode\n",
    "    [2] = Weather\n",
    "    '''\n",
    "    try:\n",
    "        logger.info('Reading Dataframes...')\n",
    "        sales_fact_df_raw = data[0]\n",
    "        logger.info(f'Sales Data has {len(sales_fact_df_raw)} records.')\n",
    "        zipcode_dim_df_raw = data[1]\n",
    "        logger.info(f'Zipcode Data has {len(zipcode_dim_df_raw)} records.')\n",
    "        weather_dim_df_raw = data[2]\n",
    "        logger.info(f'Weather Data has {len(weather_dim_df_raw)} records.')\n",
    "        \n",
    "        logger.info(f'Processing Transformations...')\n",
    "        sales_fact_df_raw.columns = sales_fact_df_raw.columns.str.lower()\n",
    "        zipcode_dim_df_raw.columns = zipcode_dim_df_raw.columns.str.lower()\n",
    "        weather_dim_df_raw.columns = weather_dim_df_raw.columns.str.lower()\n",
    "        \n",
    "        logger.info(f'Creating a PK in zipcode_dim_df_raw...')\n",
    "        zipcode_dim_df_raw.insert(0,'zipcode_id',range(1, len(zipcode_dim_df_raw) + 1))\n",
    "        zipcode_dim_df_raw['zipcode_id'] = zipcode_dim_df_raw['zipcode_id'].astype('int32')\n",
    "        zipcode_dim_df = zipcode_dim_df_raw\n",
    "\n",
    "\n",
    "        logger.info(f'Grouping weather_dim_df_raw...')\n",
    "        weather_dim_df_raw['date'] = weather_dim_df_raw['date'].dt.year\n",
    "        weather_dim_df_raw = weather_dim_df_raw.groupby(['date','zipcode']).mean().reset_index()\n",
    "        \n",
    "        logger.info(f'Adding FK zipcode_id in weather table...')\n",
    "        weather_dim_df = pd.merge(weather_dim_df_raw,zipcode_dim_df_raw,on= 'zipcode', how='left')\n",
    "        \n",
    "        logger.info(f'Dropping null zipcode_id from weather table...')\n",
    "        weather_dim_df = weather_dim_df.dropna()\n",
    "        weather_dim_df['zipcode_id'] = weather_dim_df['zipcode_id'].astype('int32')\n",
    "        \n",
    "        logger.info(f'Creating a PK in weather_dim_df_raw...')\n",
    "        weather_dim_df.insert(0,'weather_id',range(1, len(weather_dim_df) + 1))\n",
    "        weather_dim_df['weather_id'] = weather_dim_df['weather_id'].astype('int32')\n",
    "        \n",
    "        logger.info(f'Creating a PK in sales_fact_df_raw...')\n",
    "        sales_fact_df_raw.insert(0,'sales_id',range(1, len(sales_fact_df_raw) + 1))\n",
    "        sales_fact_df_raw['sales_id'] = sales_fact_df_raw['sales_id'].astype('int32')\n",
    "        \n",
    "        logger.info(f'Adding calculated column most_recent_contract_signature to sales_fact_df...')\n",
    "        sales_fact_df_raw.insert(16,'most_recent_contract_signature', \\\n",
    "            sales_fact_df_raw[['contract_1_signature_date', 'contract_2_signature_date']].max(axis=1))\n",
    "\n",
    "        logger.info(f'Adding FK zipcode_id in sales table...')\n",
    "        sales_fact_df = pd.merge(sales_fact_df_raw, zipcode_dim_df_raw, on='zipcode', how='left')\n",
    "        \n",
    "        logger.info(f'Handling column types, names and selection...')\n",
    "        weather_dim_df = weather_dim_df.rename(columns=FINAL_NAMES_WEATHER)\n",
    "        weather_dim_df = weather_dim_df[FINAL_COLS_WEATHER]\n",
    "        sales_fact_df = sales_fact_df[FINAL_COLS_SALES]        \n",
    "        \n",
    "        logger.info(f'Packing data for loading...')\n",
    "        list_of_transformed_dfs = [zipcode_dim_df,weather_dim_df, sales_fact_df]\n",
    "\n",
    "        return list_of_transformed_dfs\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f'Transformation error: {e}', exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "list_of_transformed_dfs = transform_data(list_of_dfs)\n",
    "\n",
    "@log_wrap\n",
    "def create_table(logger):\n",
    "    connection = mysql.connector.connect(\n",
    "        user = creds['mysql-db']['username'],\n",
    "        password = creds['mysql-db']['password'],\n",
    "        host = creds['mysql-db']['host'],\n",
    "        database = creds['mysql-db']['database'],\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    cursor.execute(CREATE_ZIPCODE_DIM)\n",
    "    cursor.execute(CREATE_WEATHER_DIM)\n",
    "    cursor.execute(CREATE_SALES_FACT)\n",
    "    connection.commit()\n",
    "    logger.info(\"Table structures created successfully.\")\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "dfs_dict = {\n",
    "        \"zipcode_dim\": list_of_transformed_dfs[0],\n",
    "        \"weather_dim\": list_of_transformed_dfs[1],\n",
    "        \"sales_fact\": list_of_transformed_dfs[2]\n",
    "    }\n",
    "\n",
    "@log_wrap\n",
    "def write_to_database(dfs_dict, logger, if_exists='replace'):\n",
    "    \"\"\"\n",
    "    Write a dataframe into a MySql table.\n",
    "\n",
    "    Args:\n",
    "        dfs_dict: The list of tables to load to along with the dfs to insert\n",
    "        if_exists (str): Default 'append'\n",
    "    \"\"\"\n",
    "\n",
    "    _db_user = creds['mysql-db']['username']\n",
    "    _db_password = creds['mysql-db']['password']\n",
    "    _db_host = creds['mysql-db']['host']\n",
    "    _db_name = creds['mysql-db']['database']\n",
    "    \n",
    "    engine = create_engine(f\"mysql+pymysql://{_db_user}:{_db_password}@{_db_host}:3306/{_db_name}\")\n",
    "    with engine.connect() as connection:\n",
    "        for table_name, df in dfs_dict.items():\n",
    "            if isinstance(df, pd.DataFrame): \n",
    "                df.to_sql(table_name, con=connection, if_exists=if_exists, index=False)\n",
    "                logger.info(f\"Data successfully inserted into {table_name}\")\n",
    "            else:\n",
    "                logger.info(f\"Skipping {table_name}: Not a valid DataFrame\")\n",
    "    \n",
    "    # return logger.info(\"Completed uploading all data..\")\n",
    "\n",
    "\n",
    "create_table()\n",
    "write_to_database(dfs_dict)\n",
    "\n",
    "@log_wrap\n",
    "def create_relationships(logger):\n",
    "    # Establish database connection\n",
    "    connection = mysql.connector.connect(\n",
    "        user = creds['mysql-db']['username'],\n",
    "        password = creds['mysql-db']['password'],\n",
    "        host = creds['mysql-db']['host'],\n",
    "        database = creds['mysql-db']['database'],\n",
    "    )\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    cursor.execute(ALTER_ZIPCODE_DIM)\n",
    "    cursor.execute(ALTER_WEATHER_DIM)\n",
    "    cursor.execute(ALTER_SALES_FACT_1)\n",
    "    cursor.execute(ALTER_SALES_FACT_2)\n",
    "\n",
    "    connection.commit()\n",
    "    logger.info(\"ALTER TABLE query executed successfully.\")\n",
    "\n",
    "create_relationships()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALTER_ZIPCODE_TABLE = '''\n",
    "ALTER TABLE zipcode_dim\n",
    "MODIFY COLUMN zipcode_id INT AUTO_INCREMENT PRIMARY KEY\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALTER_WEATHER_TABLE = '''\n",
    "ALTER TABLE weather_dim\n",
    "MODIFY COLUMN weather_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "ADD CONSTRAINT fk_constraint_zipcode_id FOREIGN KEY (zipcode_id) REFERENCES zipcode_dim(zipcode_id)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALTER_SALES_TABLE_1 = '''\n",
    "ALTER TABLE sales_fact\n",
    "MODIFY COLUMN zipcode_id INT\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALTER_SALES_TABLE_2 = '''\n",
    "ALTER TABLE sales_fact\n",
    "MODIFY COLUMN sales_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "ADD CONSTRAINT fk_constraint_zipcode_id_ FOREIGN KEY (zipcode_id) REFERENCES zipcode_dim(zipcode_id)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
